{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bc7f2eb-e995-4512-9e68-06681dc26b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import re\n",
    "\n",
    "from typing import List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "695f7092-6790-43f7-9177-1e2d4ea6ddd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаём внутреннюю папку проекта\n",
    "os.makedirs('pipeline', exist_ok=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a991711a-05f4-4b8a-a3bc-11faa01c8919",
   "metadata": {},
   "source": [
    "# Lists of features for the functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda82a61-1a8a-4f84-b873-d53bed7ab196",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Basic lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cd86d03c-70ae-4b3c-8a89-abb51550015f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rn - уникальный признак\n",
    "\n",
    "# Бинаризированные\n",
    "pre_features = [\n",
    "    'pre_since_opened',\n",
    "    'pre_since_confirmed',\n",
    "    'pre_pterm',\n",
    "    'pre_fterm',\n",
    "    'pre_till_pclose',\n",
    "    'pre_till_fclose',\n",
    "    'pre_loans_credit_limit',\n",
    "    'pre_loans_next_pay_summ',\n",
    "    'pre_loans_outstanding',\n",
    "    'pre_loans_max_overdue_sum',\n",
    "    'pre_loans_credit_cost_rate',\n",
    "    'pre_loans5',\n",
    "    'pre_loans530',\n",
    "    'pre_loans3060',\n",
    "    'pre_loans6090',\n",
    "    'pre_loans90',\n",
    "    'pre_util',\n",
    "    'pre_over2limit',\n",
    "    'pre_maxover2limit'\n",
    "]\n",
    "\n",
    "# Закодированные\n",
    "enc_features = [\n",
    "    'enc_loans_account_holder_type',\n",
    "    'enc_loans_credit_status',\n",
    "    'enc_loans_credit_type',\n",
    "    'enc_loans_account_cur'\n",
    "]\n",
    "\n",
    "# Статусы ежемесячных платежей\n",
    "enc_paym_features = [\n",
    "    'enc_paym_0',\n",
    "    'enc_paym_1',\n",
    "    'enc_paym_2',\n",
    "    'enc_paym_3',\n",
    "    'enc_paym_4',\n",
    "    'enc_paym_5',\n",
    "    'enc_paym_6',\n",
    "    'enc_paym_7',\n",
    "    'enc_paym_8',\n",
    "    'enc_paym_9',\n",
    "    'enc_paym_10',\n",
    "    'enc_paym_11',\n",
    "    'enc_paym_12',\n",
    "    'enc_paym_13',\n",
    "    'enc_paym_14',\n",
    "    'enc_paym_15',\n",
    "    'enc_paym_16',\n",
    "    'enc_paym_17',\n",
    "    'enc_paym_18',\n",
    "    'enc_paym_19',\n",
    "    'enc_paym_20',\n",
    "    'enc_paym_21',\n",
    "    'enc_paym_22',\n",
    "    'enc_paym_23',\n",
    "    'enc_paym_24'\n",
    "]\n",
    "\n",
    "#  Флаги\n",
    "flag_features = [\n",
    "    'is_zero_loans5',\n",
    "    'is_zero_loans530',\n",
    "    'is_zero_loans3060',\n",
    "    'is_zero_loans6090',\n",
    "    'is_zero_loans90',\n",
    "    'is_zero_util',\n",
    "    'is_zero_over2limit',\n",
    "    'is_zero_maxover2limit',\n",
    "    'pclose_flag',\n",
    "    'fclose_flag'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1809be45-ffa0-479d-ab99-ed02a40fd67d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20931476, 61)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_source = pd.read_csv('prepared_data/source_data_train_1.csv')\n",
    "df_source.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fcd5991d-f0bb-4517-a2cd-36620a1315c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2400000, 61)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result = pd.read_csv('prepared_data/cut_corr_imp_train.csv')\n",
    "df_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2bf0aa41-c0dd-4ed2-8534-66510b827c10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'rn',\n",
       " 'pre_since_opened',\n",
       " 'pre_since_confirmed',\n",
       " 'pre_pterm',\n",
       " 'pre_fterm',\n",
       " 'pre_till_pclose',\n",
       " 'pre_till_fclose',\n",
       " 'pre_loans_credit_limit',\n",
       " 'pre_loans_next_pay_summ']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_source_columns = df_source.columns.tolist()\n",
    "df_source_columns[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9cd7d089-961c-4f50-8027-fa8d06f64510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'flag',\n",
       " 'is_zero_sum_prop_1',\n",
       " 'enc_paym_avg_0_1_this_year_diff',\n",
       " 'pre_util_prop_3',\n",
       " 'enc_loans_credit_type_prop_0',\n",
       " 'pre_till_pclose_prop_10',\n",
       " 'pre_util_prop_6',\n",
       " 'pre_loans_outstanding_prop_1',\n",
       " 'pre_util_mean_freq']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result_columns = df_result.columns.tolist()\n",
    "df_result_columns[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c28d7a8-c441-4630-9174-d5e2086a0c19",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## List of features to download from the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fbf36f36-3bbf-4834-9a5d-f23dd1d5f796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['pre_loans_total_overdue',\n",
       " 'pre_loans3060',\n",
       " 'pre_loans6090',\n",
       " 'pre_loans90',\n",
       " 'is_zero_loans3060',\n",
       " 'is_zero_loans6090',\n",
       " 'is_zero_loans90',\n",
       " 'pre_maxover2limit',\n",
       " 'is_zero_util',\n",
       " 'is_zero_maxover2limit',\n",
       " 'enc_paym_3',\n",
       " 'enc_paym_4',\n",
       " 'enc_paym_5',\n",
       " 'enc_paym_6',\n",
       " 'enc_paym_7',\n",
       " 'enc_paym_11',\n",
       " 'enc_paym_12',\n",
       " 'enc_paym_13',\n",
       " 'enc_paym_14',\n",
       " 'enc_paym_15',\n",
       " 'enc_paym_16',\n",
       " 'enc_paym_17',\n",
       " 'enc_paym_18',\n",
       " 'enc_paym_19',\n",
       " 'enc_paym_20',\n",
       " 'enc_paym_21',\n",
       " 'enc_paym_22',\n",
       " 'enc_paym_23',\n",
       " 'pclose_flag',\n",
       " 'fclose_flag']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Формируем список колонок из df_source_columns,\n",
    "которые НЕ встречаются ни в одном названии из df_result_columns как подстрока.\n",
    "\"\"\"\n",
    "drop_list = []\n",
    "for col_source in df_source_columns:\n",
    "    found = False\n",
    "    for col_result in df_result_columns:\n",
    "        if col_source in col_result:\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        drop_list.append(col_source)\n",
    "\n",
    "print(len(drop_list))\n",
    "drop_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2bd47bf9-694c-4bfd-acec-fa2ae3ab047f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'rn',\n",
       " 'pre_since_opened',\n",
       " 'pre_since_confirmed',\n",
       " 'pre_pterm',\n",
       " 'pre_fterm',\n",
       " 'pre_till_pclose',\n",
       " 'pre_till_fclose',\n",
       " 'pre_loans_credit_limit',\n",
       " 'pre_loans_next_pay_summ']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "needed_columns = [x for x in df_source_columns if x not in drop_list]\n",
    "\n",
    "print(len(needed_columns))\n",
    "needed_columns[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "84483ca2-71ed-41d2-b503-624f146a9458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'rn',\n",
       " 'pre_since_opened',\n",
       " 'pre_since_confirmed',\n",
       " 'pre_pterm',\n",
       " 'pre_fterm',\n",
       " 'pre_till_pclose',\n",
       " 'pre_till_fclose',\n",
       " 'pre_loans_credit_limit',\n",
       " 'pre_loans_next_pay_summ']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Добавим недостающие признаки из групп flag_features и enc_paym _features, \n",
    "для правильной работы функций обрабатывающих эти группы. \n",
    "\"\"\"\n",
    "features_list= [\n",
    "    'is_zero_loans3060',\n",
    "    'is_zero_loans6090',\n",
    "    'is_zero_loans90',\n",
    "    'enc_paym_3',\n",
    "    'enc_paym_4',\n",
    "    'enc_paym_5',\n",
    "    'enc_paym_6',\n",
    "    'enc_paym_7',\n",
    "    'enc_paym_11',\n",
    "    'enc_paym_12',\n",
    "    'enc_paym_13',\n",
    "    'enc_paym_14',\n",
    "    'enc_paym_15',\n",
    "    'enc_paym_16',\n",
    "    'enc_paym_17',\n",
    "    'enc_paym_18',\n",
    "    'enc_paym_19',\n",
    "    'enc_paym_20',\n",
    "    'enc_paym_21',\n",
    "    'enc_paym_22',\n",
    "    'enc_paym_23'\n",
    "]\n",
    "\n",
    "# Список признаков для скачивания из исходного датасета\n",
    "needed_columns = needed_columns + features_list\n",
    "\n",
    "print(len(needed_columns))\n",
    "needed_columns[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999d67d7-625f-4cde-9d97-0a2905017239",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Create_definite_value_proportion_features_pipeline funtion list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b8918094-ea0a-43db-99db-cd070b4b5bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['is_zero_sum_prop_1',\n",
       " 'pre_util_prop_3',\n",
       " 'enc_loans_credit_type_prop_0',\n",
       " 'pre_till_pclose_prop_10',\n",
       " 'pre_util_prop_6',\n",
       " 'pre_loans_outstanding_prop_1',\n",
       " 'pre_loans_credit_limit_prop_2',\n",
       " 'pre_loans_credit_cost_rate_prop_6',\n",
       " 'pre_loans_outstanding_prop_5',\n",
       " 'pre_loans_credit_cost_rate_prop_11',\n",
       " 'pre_loans_credit_cost_rate_prop_4',\n",
       " 'pre_loans_next_pay_summ_prop_5',\n",
       " 'pre_since_opened_prop_12',\n",
       " 'pre_loans_credit_limit_prop_15',\n",
       " 'enc_loans_credit_type_prop_2',\n",
       " 'pre_fterm_prop_7',\n",
       " 'enc_paym_0_prop_1',\n",
       " 'is_zero_over2limit_prop_1',\n",
       " 'pre_since_opened_prop_8',\n",
       " 'pre_loans_max_overdue_sum_prop_1',\n",
       " 'pre_loans_next_pay_summ_prop_0',\n",
       " 'pre_pterm_prop_6',\n",
       " 'pre_since_opened_prop_19',\n",
       " 'is_zero_loans5_prop_1',\n",
       " 'enc_loans_account_holder_type_prop_4',\n",
       " 'pre_loans_credit_limit_prop_18',\n",
       " 'pre_till_fclose_prop_4',\n",
       " 'pre_pterm_prop_3',\n",
       " 'is_zero_loans530_prop_1',\n",
       " 'enc_loans_credit_status_prop_5',\n",
       " 'pre_since_confirmed_prop_4',\n",
       " 'pre_fterm_prop_3',\n",
       " 'pre_till_fclose_prop_3',\n",
       " 'pre_till_fclose_prop_1',\n",
       " 'pre_till_pclose_prop_7',\n",
       " 'pre_since_confirmed_prop_7',\n",
       " 'enc_paym_24_prop_1',\n",
       " 'pre_over2limit_prop_17']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Создадим список пропорциональных фичей в итоговом датасете\n",
    "prop_features_result_list = [col for col in df_result_columns if 'prop_' in col]\n",
    "\n",
    "print(len(prop_features_result_list))\n",
    "prop_features_result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "864d11b8-b29a-4cf7-8e75-1c82fa30f7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['pre_till_pclose',\n",
       " 'is_zero_loans5',\n",
       " 'pre_since_opened',\n",
       " 'enc_loans_credit_type',\n",
       " 'pre_since_confirmed',\n",
       " 'pre_fterm',\n",
       " 'pre_till_fclose',\n",
       " 'enc_paym_24',\n",
       " 'enc_paym_0',\n",
       " 'enc_loans_credit_status',\n",
       " 'pre_loans_max_overdue_sum',\n",
       " 'enc_loans_account_holder_type',\n",
       " 'pre_loans_outstanding',\n",
       " 'pre_loans_credit_cost_rate',\n",
       " 'is_zero_over2limit',\n",
       " 'pre_util',\n",
       " 'pre_loans_next_pay_summ',\n",
       " 'is_zero_sum',\n",
       " 'pre_loans_credit_limit',\n",
       " 'pre_over2limit',\n",
       " 'is_zero_loans530',\n",
       " 'pre_pterm']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Создадим список признаков исходного датасета из которых были сделаны пропорциональные фичи\n",
    "prop_features_source_list = list(\n",
    "    set(\n",
    "        [\n",
    "            re.sub(r'_prop.*$', '', col)\n",
    "            for col in prop_features_result_list\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "print(len(prop_features_source_list))\n",
    "prop_features_source_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3c5e70c9-1efe-4c0d-ab60-9e90553339f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pre_till_pclose': [10, 7],\n",
       " 'is_zero_loans5': [1],\n",
       " 'pre_since_opened': [12, 8, 19],\n",
       " 'enc_loans_credit_type': [0, 2],\n",
       " 'pre_since_confirmed': [4, 7],\n",
       " 'pre_fterm': [7, 3],\n",
       " 'pre_till_fclose': [4, 3, 1],\n",
       " 'enc_paym_24': [1],\n",
       " 'enc_paym_0': [1],\n",
       " 'enc_loans_credit_status': [5],\n",
       " 'pre_loans_max_overdue_sum': [1],\n",
       " 'enc_loans_account_holder_type': [4],\n",
       " 'pre_loans_outstanding': [1, 5],\n",
       " 'pre_loans_credit_cost_rate': [6, 11, 4],\n",
       " 'is_zero_over2limit': [1],\n",
       " 'pre_util': [3, 6],\n",
       " 'pre_loans_next_pay_summ': [5, 0],\n",
       " 'is_zero_sum': [1],\n",
       " 'pre_loans_credit_limit': [2, 15, 18],\n",
       " 'pre_over2limit': [17],\n",
       " 'is_zero_loans530': [1],\n",
       " 'pre_pterm': [6, 3]}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Соберем часть словаря пропорциональных фичей для пайплайна\n",
    "prop_features_dict = {}\n",
    "\n",
    "for source_col in prop_features_source_list:\n",
    "    # Инициализируем пустой список для каждого исходного признака\n",
    "    prop_features_dict[source_col] = []\n",
    "    # Создадим паттерн: имя col в начале и после него подчёркивание или конец строки\n",
    "    pattern = re.compile(r'^' + source_col + r'(_|$)')\n",
    "    for result_col in prop_features_result_list:\n",
    "        # Проверяем, совпадает ли имя признака с паттерном\n",
    "        if pattern.match(result_col):\n",
    "            # Ищем число в конце строки\n",
    "            match = re.search(r'(\\d+)$', result_col)\n",
    "            # Добавляем найденное число в список для данного source_col\n",
    "            prop_features_dict[source_col].append(int(match.group(1)))\n",
    "prop_features_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6776ea8b-4970-440c-b8ac-c93930ca9652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pre_till_pclose': [10, 7],\n",
       " 'is_zero_loans5': [1],\n",
       " 'pre_since_opened': [12, 8, 19],\n",
       " 'enc_loans_credit_type': [0, 2],\n",
       " 'pre_since_confirmed': [4, 7],\n",
       " 'pre_fterm': [7, 3],\n",
       " 'pre_till_fclose': [4, 3, 1],\n",
       " 'enc_paym_24': [1],\n",
       " 'enc_paym_0': [1],\n",
       " 'enc_loans_credit_status': [5],\n",
       " 'pre_loans_max_overdue_sum': [1],\n",
       " 'enc_loans_account_holder_type': [4],\n",
       " 'pre_loans_outstanding': [1, 5],\n",
       " 'pre_loans_credit_cost_rate': [6, 11, 4],\n",
       " 'is_zero_over2limit': [1],\n",
       " 'pre_util': [3, 6],\n",
       " 'pre_loans_next_pay_summ': [5, 0],\n",
       " 'pre_loans_credit_limit': [2, 15, 18],\n",
       " 'pre_over2limit': [17],\n",
       " 'is_zero_loans530': [1],\n",
       " 'pre_pterm': [6, 3],\n",
       " 'is_zero_loans3060': [1],\n",
       " 'is_zero_loans6090': [1],\n",
       " 'is_zero_loans90': [1]}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Добавим в словарь недостающие is_zero_loans* для функции суммирования.\n",
    "Удалим is_zero_sum, фича is_zero_sum_prop_1 будет собираться другой функцией.\n",
    "\"\"\"\n",
    "is_zero_loans_list = [\n",
    "        'is_zero_loans5',\n",
    "        'is_zero_loans530',\n",
    "        'is_zero_loans3060',\n",
    "        'is_zero_loans6090',\n",
    "        'is_zero_loans90'\n",
    "    ]\n",
    "for col in is_zero_loans_list:\n",
    "    if col not in prop_features_dict.keys():\n",
    "        prop_features_dict[col] = [1]\n",
    "        \n",
    "del prop_features_dict['is_zero_sum']\n",
    "\n",
    "prop_features_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0a3e3f-a04f-4cb9-a28e-309b435f2cb2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## List for create_mean_value_frequency_feature_pipeline features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e3344e24-9ffb-45ca-ab41-0f8e2ee01700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['pre_util_mean_freq',\n",
       " 'pre_loans_credit_limit_mean_freq',\n",
       " 'pre_since_opened_mean_freq',\n",
       " 'pre_loans_credit_cost_rate_mean_freq',\n",
       " 'enc_loans_credit_type_mean_freq',\n",
       " 'pre_loans_next_pay_summ_mean_freq',\n",
       " 'pre_since_confirmed_mean_freq',\n",
       " 'pre_pterm_mean_freq',\n",
       " 'enc_paym_0_mean_freq',\n",
       " 'enc_loans_account_holder_type_mean_freq',\n",
       " 'pre_loans530_mean_freq',\n",
       " 'enc_paym_8_mean_freq',\n",
       " 'pre_loans5_mean_freq',\n",
       " 'enc_paym_10_mean_freq',\n",
       " 'enc_loans_account_cur_mean_freq',\n",
       " 'enc_paym_9_mean_freq']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Соберем список всех фичей средней частотности в итоговом датасете\n",
    "mean_freq_result_list = [col for col in df_result_columns if 'mean_freq' in col]\n",
    "\n",
    "print(len(mean_freq_result_list))\n",
    "mean_freq_result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1ec18ab2-0503-4a84-8dd3-78be3f1457f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['pre_util',\n",
       " 'pre_loans_credit_limit',\n",
       " 'pre_since_opened',\n",
       " 'pre_loans_credit_cost_rate',\n",
       " 'enc_loans_credit_type',\n",
       " 'pre_loans_next_pay_summ',\n",
       " 'pre_since_confirmed',\n",
       " 'pre_pterm',\n",
       " 'enc_paym_0',\n",
       " 'enc_loans_account_holder_type',\n",
       " 'pre_loans530',\n",
       " 'enc_paym_8',\n",
       " 'pre_loans5',\n",
       " 'enc_paym_10',\n",
       " 'enc_loans_account_cur',\n",
       " 'enc_paym_9']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Соберем список признаков исходного датасета \n",
    "из которых были сделаны фичи средней частотности.\n",
    "\"\"\"\n",
    "mean_freq_source_list = [x[:-len('_mean_freq')] for x in mean_freq_result_list]\n",
    "print(len(mean_freq_source_list))\n",
    "mean_freq_source_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b72342-98ab-45a1-842d-96c6a2b3de50",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Drop list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "52afd524-3819-4222-8fcb-e2f1bb21ff46",
   "metadata": {},
   "outputs": [],
   "source": [
    "temporary_features_list = [\n",
    "    'enc_paym_avg_1_all',\n",
    "    'enc_paym_avg_2_all',\n",
    "    'enc_paym_avg_0_this_year',\n",
    "    'enc_paym_avg_1_this_year',\n",
    "    'enc_paym_avg_0_last_year',\n",
    "    'is_zero_loans3060_prop_1',\n",
    "    'is_zero_loans6090_prop_1',\n",
    "    'is_zero_loans90_prop_1'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cf48c49b-bd63-40eb-9a6f-32f1e9a96fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'rn',\n",
       " 'pre_since_opened',\n",
       " 'pre_since_confirmed',\n",
       " 'pre_pterm',\n",
       " 'pre_fterm',\n",
       " 'pre_till_pclose',\n",
       " 'pre_till_fclose',\n",
       " 'pre_loans_credit_limit',\n",
       " 'pre_loans_next_pay_summ',\n",
       " 'pre_loans_outstanding',\n",
       " 'pre_loans_max_overdue_sum',\n",
       " 'pre_loans_credit_cost_rate',\n",
       " 'pre_loans5',\n",
       " 'pre_loans530',\n",
       " 'is_zero_loans5',\n",
       " 'is_zero_loans530',\n",
       " 'pre_util',\n",
       " 'pre_over2limit',\n",
       " 'is_zero_over2limit',\n",
       " 'enc_paym_0',\n",
       " 'enc_paym_1',\n",
       " 'enc_paym_2',\n",
       " 'enc_paym_8',\n",
       " 'enc_paym_9',\n",
       " 'enc_paym_10',\n",
       " 'enc_paym_24',\n",
       " 'enc_loans_account_holder_type',\n",
       " 'enc_loans_credit_status',\n",
       " 'enc_loans_credit_type',\n",
       " 'enc_loans_account_cur',\n",
       " 'is_zero_loans3060',\n",
       " 'is_zero_loans6090',\n",
       " 'is_zero_loans90',\n",
       " 'enc_paym_3',\n",
       " 'enc_paym_4',\n",
       " 'enc_paym_5',\n",
       " 'enc_paym_6',\n",
       " 'enc_paym_7',\n",
       " 'enc_paym_11',\n",
       " 'enc_paym_12',\n",
       " 'enc_paym_13',\n",
       " 'enc_paym_14',\n",
       " 'enc_paym_15',\n",
       " 'enc_paym_16',\n",
       " 'enc_paym_17',\n",
       " 'enc_paym_18',\n",
       " 'enc_paym_19',\n",
       " 'enc_paym_20',\n",
       " 'enc_paym_21',\n",
       " 'enc_paym_22',\n",
       " 'enc_paym_23',\n",
       " 'enc_paym_avg_1_all',\n",
       " 'enc_paym_avg_2_all',\n",
       " 'enc_paym_avg_0_this_year',\n",
       " 'enc_paym_avg_1_this_year',\n",
       " 'enc_paym_avg_0_last_year',\n",
       " 'is_zero_loans3060_prop_1',\n",
       " 'is_zero_loans6090_prop_1',\n",
       " 'is_zero_loans90_prop_1']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_list = needed_columns + temporary_features_list\n",
    "print(len(drop_list))\n",
    "drop_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3774b5-4a4d-417c-972e-6f998596816f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Downloading dataset and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cf6d693b-65ab-4f66-b7fd-93020e230eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# СКАЧИВАЕМ ИСХОДНЫЙ ДАТАСЕТ\n",
    "\n",
    "def read_parquet_dataset_from_local(\n",
    "    path_to_dataset: str,\n",
    "    start_from: int = 0,\n",
    "    num_parts_to_read: int = 2,\n",
    "    columns: Optional[List[str]] = None,\n",
    "    verbose: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Читает num_parts_to_read партиций, преобразовывает их к pd.DataFrame и возвращает.\n",
    "\n",
    "    Args:\n",
    "        path_to_dataset : путь до директории с партициями\n",
    "        start_from : номер партиции, с которой нужно начать чтение\n",
    "        num_parts_to_read : количество партиций, которые требуется прочитать\n",
    "        columns : список колонок, которые нужно прочитать из партиции\n",
    "        verbose : выводить ли дополнительную информацию\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame \n",
    "    \"\"\"\n",
    "    res = []\n",
    "    dataset_paths = sorted(\n",
    "        os.path.join(path_to_dataset, filename)\n",
    "        for filename in os.listdir(path_to_dataset)\n",
    "        if filename.startswith('train')\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        print('Dataset paths:')\n",
    "        for path in dataset_paths:\n",
    "            print(path)\n",
    "\n",
    "    start_from = max(0, start_from)\n",
    "    chunks = dataset_paths[start_from: start_from + num_parts_to_read]\n",
    "\n",
    "    if verbose:\n",
    "        print('Reading chunks:')\n",
    "        for chunk in chunks:\n",
    "            print(chunk)\n",
    "\n",
    "    for chunk_path in tqdm(chunks, desc=\"Reading dataset with pandas\"):\n",
    "        if verbose:\n",
    "            print('Reading chunk:', chunk_path)\n",
    "        chunk = pd.read_parquet(chunk_path, columns=columns)\n",
    "        res.append(chunk)\n",
    "\n",
    "    return pd.concat(res).reset_index(drop=True)\n",
    "\n",
    "def prepare_transactions_dataset(\n",
    "    path_to_dataset: str,\n",
    "    num_parts_to_preprocess_at_once: int = 1,\n",
    "    num_parts_total: int = 50,\n",
    "    save_to_path: str = None,\n",
    "    verbose: bool = False,\n",
    "    columns: Optional[List[str]] = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Возвращает исходный pd.DataFrame с признаками из которых нужно собрать\n",
    "    учебный датасет.\n",
    "\n",
    "    Args:\n",
    "        path_to_dataset : путь до датасета с партициями\n",
    "        num_parts_to_preprocess_at_once : количество партиций, \n",
    "            которые будут одновременно держаться и обрабатываться в памяти\n",
    "        num_parts_total : общее количество партиций, которые нужно обработать\n",
    "        save_to_path : путь до папки для сохранения обработанных блоков в .parquet-формате; \n",
    "            если None, сохранение не происходит\n",
    "        verbose : логировать каждую обрабатываемую часть данных\n",
    "        columns : список колонок, которые нужно оставить\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame : датафрейм с объединёнными данными\n",
    "    \"\"\"\n",
    "    preprocessed_frames = []\n",
    "\n",
    "    for step in tqdm(range(0, num_parts_total, num_parts_to_preprocess_at_once),\n",
    "                     desc=\"Transforming transactions data\"):\n",
    "        transactions_frame = read_parquet_dataset_from_local(\n",
    "            path_to_dataset,\n",
    "            start_from=step,\n",
    "            num_parts_to_read=num_parts_to_preprocess_at_once,\n",
    "            verbose=verbose,\n",
    "            columns=columns\n",
    "        )\n",
    "\n",
    "       # Записываем подготовленные данные в файл\n",
    "        if save_to_path:\n",
    "            block_as_str = str(step)\n",
    "            if len(block_as_str) == 1:\n",
    "                block_as_str = '00' + block_as_str\n",
    "            else:\n",
    "                block_as_str = '0' + block_as_str\n",
    "            transactions_frame.to_parquet(os.path.join(save_to_path, f'processed_chunk_{block_as_str}.parquet'))\n",
    "\n",
    "        preprocessed_frames.append(transactions_frame)\n",
    "    \n",
    "    return pd.concat(preprocessed_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "938b8752-54e0-415c-9c1a-5345f1fe9210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e75eef828634b33b19fc6788d860e95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transforming transactions data:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53835feaec37409da6c098cc4be665fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd762ebb46344431913b494021074fe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28e2e4cfda8e4aaba91d602a6be16968",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8ad74d9bdb54983a043bb08d2de7530",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a51fb18b57b24c51a594356b2d93030a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75ed75073a6c4ecca78c846f91c5db11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6829b75940384b5b8be369c21752d502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14da6862958d43edacc381abc97650a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c2e9cdc70d94f46b3b1dc4fd337b0de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a38ce4f0814a4ea09c7aeccb44bc56e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5f8f8a9e92d4b9a826e46e6da972afa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8caf252088274ae682526e21e9ac98e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "((20931476, 52), (5231241, 52), (2400000,), (600000,))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Собираем исходный датасет из parquet файлов,  \n",
    "скачиваем только необходимые колонки\n",
    "\"\"\"\n",
    "# Путь до данных в проекте\n",
    "path = 'train_data/'\n",
    "\n",
    "data = prepare_transactions_dataset(\n",
    "    path,\n",
    "    num_parts_to_preprocess_at_once=1,\n",
    "    num_parts_total=12,\n",
    "    save_to_path='train_data/',\n",
    "    columns=needed_columns) \n",
    "\n",
    "# Загружаем датасет с целевой переменной\n",
    "target = pd.read_csv('train_target.csv')\n",
    "\n",
    "# Делим датасет с целевой переменной на train/test части\n",
    "y_train, y_test  = train_test_split(target, train_size=0.8, random_state=0, stratify=target.flag)\n",
    "\n",
    "# Забираем наборы id из train/test\n",
    "train_id = y_train['id'].values\n",
    "test_id = y_test['id'].values\n",
    "\n",
    "# На основе наборов id делим исходный датасет на train/test части\n",
    "X_train = data.set_index('id').loc[train_id].reset_index()\n",
    "X_test = data.set_index('id').loc[test_id].reset_index()\n",
    "\n",
    "# Сбросим индексы для приведения к единому виду с X_train/X_test \n",
    "y_train = y_train.reset_index(drop=True)['flag']\n",
    "y_test = y_test.reset_index(drop=True)['flag']\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "0392a984-602b-4c1a-a93f-17f0bba0c7da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20931476, 52), (5231241, 52), (2400000,), (600000,))"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Сохраним разделённые данные\n",
    "X_train.to_csv('pipeline/X_train.csv', index=False)\n",
    "X_test.to_csv('pipeline/X_test.csv', index=False)\n",
    "y_train.to_csv('pipeline/y_train.csv', index=False)\n",
    "y_test.to_csv('pipeline/y_test.csv', index=False)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30275359-2d00-4740-bb33-b80e8f7ba866",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de04b0b-97a3-4d55-a307-607e2b256633",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030cda95-9d82-49ff-a878-adcfd4a3c301",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0cd7962-3572-4930-99d7-c51655fe80de",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44231b7d-9226-4aad-a823-ffcbe7db8e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20931476, 52), (5231241, 52), (2400000, 1), (600000, 1))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загружаем исходные разделённые данные \n",
    "X_train = pd.read_csv('pipeline/X_train.csv')\n",
    "X_test = pd.read_csv('pipeline/X_test.csv')\n",
    "y_train = pd.read_csv('pipeline/y_train.csv')\n",
    "y_test = pd.read_csv('pipeline/y_test.csv')\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4feb7646-a345-45b7-8bba-252d9a5af26d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6015c69-9e80-43fd-899c-5f7359fb8223",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8d69930a-45f8-4f89-b991-bfa15ed84c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20931476, 52)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Для полной проверки будем использовать копию тренировочных данных.\n",
    "\"\"\"\n",
    "X_train_full = X_train.copy()\n",
    "X_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2585a60f-ea6e-46d4-a562-c3662858e5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA PREPROCESSING\n",
    "\n",
    "def convert_all_to_numeric(\n",
    "    df: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Преобразует типы всех колоноки в числовые с заменой ошибок на NaN.\n",
    "\n",
    "    Args:\n",
    "        df: Исходный DataFrame, содержащий колонки 'id' и 'rn'.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: Копия исходного DataFrame с добавленной колонкой 'rn_max'.\n",
    "    \"\"\"\n",
    "    return df.apply(lambda col: pd.to_numeric(col, errors='coerce'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e3290d14-eac0-4546-a36a-1c5e59851111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE ENGENERING FUNCTIONS\n",
    "\n",
    "def create_rn_max_feature_pipeline(\n",
    "    df: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Добавляет в DataFrame новую колонку 'rn_max' — максимальное \n",
    "    значение 'rn' для каждой группы 'id'.\n",
    "\n",
    "    Args:\n",
    "        df: Исходный DataFrame, содержащий колонки 'id' и 'rn'.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: Копия исходного DataFrame с добавленной колонкой 'rn_max'.\n",
    "    \"\"\"\n",
    "    print('FUNCTION create_rn_max_feature ')\n",
    "    df = df.copy()\n",
    "    # Вычисляем максимальное значение 'rn' для каждой группы 'id'\n",
    "    group_value = df.groupby('id')['rn'].max().rename('rn_max')\n",
    "\n",
    "    # Объедииняем исходный DataFrame с результатом группировки по 'id'\n",
    "    df = df.merge(group_value, on='id', how='left')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def enc_paym_transcoding_pipeline(\n",
    "    df: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\" \n",
    "    Прекодирует признаки enc_paym_features к единому виду с диапазоном значений {0, 1, 2, 3}.\n",
    "    Для каждого столбца enc_paym_0, enc_paym_1, ..., enc_paym_24, \n",
    "    если в значениях встречается 4, происходит замена:\n",
    "        1 -> 0\n",
    "        2 -> 1\n",
    "        3 -> 2\n",
    "        4 -> 3\n",
    "\n",
    "    Args:\n",
    "        df: Исходный DataFrame с колонками 'enc_paym_0' ... 'enc_paym_24'.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: Копия DataFrame с перекодированными признаками.\n",
    "    \"\"\"\n",
    "    print('FUNCTION enc_paym_transcoding ')\n",
    "    df = df.copy()\n",
    "    # Список колонок для перекодировки\n",
    "    columns = [f'enc_paym_{i}' for i in range(25)]\n",
    "    \n",
    "    for col in columns:\n",
    "        # Проверяем, есть ли значение 4 в колонке\n",
    "        if 4 in df[col].unique():\n",
    "            # Заменяем значения согласно маппингу\n",
    "            df.loc[:, col] = df[col].replace({1: 0, 2: 1, 3: 2, 4: 3})\n",
    "            \n",
    "    return df\n",
    "\n",
    "def create_definite_value_proportion_features_pipeline(\n",
    "    df: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Создаёт и добавляет в датафрейм новые частотные признаки \n",
    "    на основе заданных значений исходных признаков.\n",
    "    \n",
    "    Для каждого столбца и каждого указанного значения в словаре функция создаёт новые признаки, \n",
    "    отражающие долю записей с этим значением относительно общего количества \n",
    "    кредитов (rn_max) для каждого id.\n",
    "    \n",
    "    Args:\n",
    "        df: Исходный DataFrame, содержащий необходимые признаки и колонку 'rn_max'.\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: Копия исходного DataFrame с добавленными частотными признаками.\n",
    "    \"\"\"\n",
    "    print('FUNCTION create_definite_value_proportion_features ')\n",
    "    df = df.copy()\n",
    "    \n",
    "    features_dictionary = {\n",
    "        'enc_loans_account_holder_type': [4],\n",
    "        'pre_pterm': [6, 3],\n",
    "        'is_zero_loans530': [1],\n",
    "        'enc_paym_0': [1],\n",
    "        'pre_loans_credit_cost_rate': [6, 11, 4],\n",
    "        'pre_loans_next_pay_summ': [5, 0],\n",
    "        'is_zero_over2limit': [1],\n",
    "        'pre_loans_outstanding': [1, 5],\n",
    "        'pre_util': [3, 6],\n",
    "        'pre_till_pclose': [10, 7],\n",
    "        'is_zero_loans5': [1],\n",
    "        'pre_since_confirmed': [4, 7],\n",
    "        'pre_loans_credit_limit': [2, 15, 18],\n",
    "        'pre_over2limit': [17],\n",
    "        'pre_till_fclose': [4, 3, 1],\n",
    "        'enc_loans_credit_status': [5],\n",
    "        'pre_since_opened': [12, 8, 19],\n",
    "        'enc_paym_24': [1],\n",
    "        'pre_loans_max_overdue_sum': [1],\n",
    "        'enc_loans_credit_type': [0, 2],\n",
    "        'pre_fterm': [7, 3],\n",
    "        'is_zero_loans3060': [1],\n",
    "        'is_zero_loans6090': [1],\n",
    "        'is_zero_loans90': [1]\n",
    "    }   \n",
    "\n",
    "    \n",
    "        \n",
    "    for col in  features_dictionary.keys():\n",
    "        print('Исходный признак', col)\n",
    "        print('Новые фичи')\n",
    "        \n",
    "        for value in features_dictionary[col]:\n",
    "            new_column = f'{col}_prop_{value}'\n",
    "            print(new_column)                     \n",
    "            \n",
    "            # Создаём группировку с количеством value для каждого id\n",
    "            group_value = df[df[col] == value].groupby('id').size().rename(new_column)\n",
    "\n",
    "            # Объедииняем группировку с  датасетом\n",
    "            df = df.merge(group_value, on='id', how='left')\n",
    "            \n",
    "            # Заполняем пропуски нулями\n",
    "            df[new_column] = df[new_column].fillna(0)\n",
    "            \n",
    "            # Считаем отношение к количеству кредитов\n",
    "            df[new_column] = df[new_column] / df['rn_max']         \n",
    "\n",
    "    return df\n",
    "\n",
    "def from_is_zero_prop_1_create_sum_prop_1_feature_pipeline(\n",
    "    df: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Вычисляет среднее значение признаков is_zero_*_prop_1 по строкам и добавляет \n",
    "    новый признак 'is_zero_sum_prop_1' в DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df:  Исходный DataFrame с признаками is_zero_*_prop_1.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: Копия DataFrame с добавленным признаком 'is_zero_sum_prop_1'.\n",
    "    \"\"\"\n",
    "\n",
    "    print('FUNCTION of_is_zero_prop_1_create_sum_prop_1_feature ')\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    columns = [\n",
    "        'is_zero_loans5_prop_1',\n",
    "        'is_zero_loans530_prop_1',\n",
    "        'is_zero_loans3060_prop_1',\n",
    "        'is_zero_loans6090_prop_1',\n",
    "        'is_zero_loans90_prop_1'\n",
    "    ]\n",
    "\n",
    "    df['is_zero_sum_prop_1'] = df[columns].sum(axis=1) / 5\n",
    "\n",
    "    return df\n",
    "\n",
    "def create_mean_value_frequency_feature_pipeline(\n",
    "    df: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Cоздаёт новые агрегированные признаки,\n",
    "    отражающий среднюю частоту (относительную встречаемость) значений \n",
    "    заданных столбцов columns_list датафрейма для каждого уникального id.\n",
    "    Результат добавляется в  датафрейм \n",
    "    с нормировкой на количество записей (rn_max) для каждого id.\n",
    "    \n",
    "    Args:\n",
    "        df :  Исходный DataFrame с признаками из columns_list.\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame :  Копия DataFrame с добавленным новым столбцом {column}_mean_freq,\n",
    "        содержащим нормированное агрегированное значение средней \n",
    "        частоты значений column для каждого id.\n",
    "    \"\"\"\n",
    "    print('FUNCTION create_mean_value_frequency_feature ')\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # Список столбцов, для которых считаем среднюю частоту значений\n",
    "    columns_list = [\n",
    "        'pre_util',\n",
    "        'pre_loans_credit_limit',\n",
    "        'pre_since_opened',\n",
    "        'pre_loans_credit_cost_rate',\n",
    "        'enc_loans_credit_type',\n",
    "        'pre_loans_next_pay_summ',\n",
    "        'pre_since_confirmed',\n",
    "        'pre_pterm',\n",
    "        'enc_paym_0',\n",
    "        'enc_loans_account_holder_type',\n",
    "        'pre_loans530',\n",
    "        'enc_paym_8',\n",
    "        'pre_loans5',\n",
    "        'enc_paym_10',\n",
    "        'enc_loans_account_cur',\n",
    "        'enc_paym_9'\n",
    "    ]\n",
    "    \n",
    "    for col in columns_list:\n",
    "        new_column = f'{col}_mean_freq'\n",
    "        print('new_column', new_column)\n",
    "        \n",
    "        # Вычисляем относительную частоту каждого уникального значения в столбце\n",
    "        bin_freq = df[col].value_counts(normalize=True).to_dict()\n",
    "        \n",
    "        # Создаём Series с частотами значений для каждой строки\n",
    "        freq_series = df[col].map(bin_freq)\n",
    "        \n",
    "        # Группируем по 'id' и суммируем частоты значений\n",
    "        agg_freq = freq_series.groupby(df['id']).sum().reset_index(name=new_column)\n",
    "        \n",
    "        # Добавляем новый признак в DataFrame, объединяя по 'id'\n",
    "        df = df.merge(agg_freq, on='id', how='left')\n",
    "    \n",
    "        # Нормируем агрегированные суммы частот на количество записей 'rn_max' для каждого id\n",
    "        df[new_column] = df[new_column] / df['rn_max']\n",
    "\n",
    "    return df\n",
    "\n",
    "def enc_paym_norm_group_sum_diff_pipeline(\n",
    "    df: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\" \n",
    "    Генерирует признаки разницы между средними количествами различных статусов платежей \n",
    "    по кредитам за разные временные промежутки.\n",
    "\n",
    "    Основная цель функции — создать итоговые признаки:\n",
    "        - 'enc_paym_avg_0_1_this_year_diff'\n",
    "        - 'enc_paym_avg_1_2_all_diff'\n",
    "        - 'enc_paym_avg_0_years_diff'\n",
    "\n",
    "    Для их расчёта временно создаются промежуточные агрегированные признаки среднего \n",
    "    количества статусов платежей по id и периоду \n",
    "    (например, 'enc_paym_avg_0_this_year'), \n",
    "    которые впоследствии удаляются из итогового датасета.\n",
    "\n",
    "    Args:\n",
    "        df :  Исходный DataFrame с признаками из columns_list.\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame :  Копия DataFrame с добавленными итоговыми признаками \n",
    "        разницы между средними количествами статусов платежей по различным периодам.\n",
    "    \"\"\"\n",
    "\n",
    "    print('FUNCTION enc_paym_norm_group_sum_diff_pipeline ')\n",
    "    df = df.copy()\n",
    "\n",
    "    # Создаём временный датафрейм со столбцом id из df\n",
    "    df_buff = pd.DataFrame(data = df['id'], columns = ['id'])\n",
    "    \n",
    "    # Временной промежуток 'all' — все периоды\n",
    "    time_span = 'all'\n",
    "    columns = [f'enc_paym_{i}' for i in range(25)]\n",
    "\n",
    "    # Для статусов платежей по кредитам 1 и 2\n",
    "    for i in range(1, 3):\n",
    "        new_col = f'enc_paym_avg_{i}_{time_span}'\n",
    "        print('new_column', new_col)\n",
    "        \n",
    "        # Считаем количество статуса i по всем столбцам за период \n",
    "        df_buff[new_col] = np.sum(\n",
    "            [df[col] == i for col in columns],\n",
    "            axis=0\n",
    "        )\n",
    "        \n",
    "        # Группируем по id и суммируем значения\n",
    "        agg_sum = (\n",
    "            df_buff\n",
    "            .groupby('id')\n",
    "            [new_col]  \n",
    "            .sum()\n",
    "            .reset_index(name=new_col)\n",
    "        )\n",
    "        \n",
    "        # Добавляем группировку в исходный DataFrame\n",
    "        df = df.merge(agg_sum, on='id', how='left')\n",
    "        \n",
    "        # Нормируем сумму на количество записей 'rn_max' \n",
    "        df[new_col] = df[new_col] / df['rn_max']\n",
    "        \n",
    "    # Временной промежуток 'this_year' — первые 12 месяцев\n",
    "    time_span = 'this_year'\n",
    "    columns = [f'enc_paym_{i}' for i in range(12)]\n",
    "\n",
    "    # Для статусов платежей по кредитам 0 и 1\n",
    "    for i in range(2):\n",
    "        new_col = f'enc_paym_avg_{i}_{time_span}'\n",
    "        print('new_column', new_col)\n",
    "        \n",
    "        # Считаем количество статуса i по всем столбцам за период \n",
    "        df_buff[new_col] = np.sum(\n",
    "            [df[col] == i for col in columns],\n",
    "            axis=0\n",
    "        )\n",
    "        \n",
    "        # Группируем по id и суммируем значения\n",
    "        agg_sum = (\n",
    "            df_buff\n",
    "            .groupby('id')\n",
    "            [new_col]  \n",
    "            .sum()\n",
    "            .reset_index(name=new_col)\n",
    "        )\n",
    "        \n",
    "        # Добавляем группировку в исходный DataFrame\n",
    "        df = df.merge(agg_sum, on='id', how='left')\n",
    "        \n",
    "        # Нормируем сумму на количество записей 'rn_max' \n",
    "        df[new_col] = df[new_col] / df['rn_max']\n",
    "        \n",
    "    # Временной промежуток 'last_year' — месяцы с 12 по 24\n",
    "    time_span = 'last_year'\n",
    "    columns = [f'enc_paym_{i}' for i in range(12, 25)]\n",
    "    \n",
    "    \"\"\"\n",
    "    Статус платежей  0.\n",
    "    (Оставим цикл для единообразия кода)\n",
    "    \"\"\"\n",
    "    for i in [0]:\n",
    "        new_col = f'enc_paym_avg_{i}_{time_span}'\n",
    "        print('new_column', new_col)\n",
    "        \n",
    "        # Считаем количество статуса i по всем столбцам за период \n",
    "        df_buff[new_col] = np.sum(\n",
    "            [df[old_col] == i for old_col in columns],\n",
    "            axis=0\n",
    "        )\n",
    "        \n",
    "        # Группируем по id и суммируем значения\n",
    "        agg_sum = (\n",
    "            df_buff\n",
    "            .groupby('id')\n",
    "            [new_col]  \n",
    "            .sum()\n",
    "            .reset_index(name=new_col)\n",
    "        )\n",
    "        \n",
    "        # Добавляем группировку в исходный DataFrame\n",
    "        df = df.merge(agg_sum, on='id', how='left')\n",
    "        \n",
    "        # Нормируем сумму на количество записей 'rn_max' \n",
    "        df[new_col] = df[new_col] / df['rn_max']\n",
    "\n",
    "    # Создаём фичи разницы \n",
    "    df['enc_paym_avg_0_1_this_year_diff'] = (\n",
    "            df['enc_paym_avg_0_this_year'] - \n",
    "            df['enc_paym_avg_1_this_year']\n",
    "    )\n",
    "\n",
    "    df['enc_paym_avg_1_2_all_diff'] = (\n",
    "            df['enc_paym_avg_1_all'] - \n",
    "            df['enc_paym_avg_2_all']\n",
    "    )\n",
    "\n",
    "    df['enc_paym_avg_0_years_diff'] = (\n",
    "            df['enc_paym_avg_0_this_year'] - \n",
    "            df['enc_paym_avg_0_last_year']\n",
    "    )\n",
    "    \n",
    "    print('new diff columns')\n",
    "    print('enc_paym_avg_0_1_this_year_diff')\n",
    "    print('enc_paym_avg_1_2_all_diff')\n",
    "    print('enc_paym_avg_0_years_diff')\n",
    "\n",
    "    return df\n",
    "\n",
    "def pre_since_opened_sum_mean_repeated_pipeline(\n",
    "    df: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Cоздаёт признак, отражающий пропорцию повторяющихся значений 'pre_since_opened'\n",
    "    для каждого 'id'.\n",
    "\n",
    "    Логика работы:\n",
    "    - Подсчитывает количество появлений каждого значения 'pre_since_opened' для каждого 'id'.\n",
    "    - Выделяет только повторяющиеся значения (где количество > 1) и вычитает 1,\n",
    "      чтобы не считать первое появление.\n",
    "    - Суммирует количество повторов по всем значениям 'pre_since_opened' для каждого 'id'.\n",
    "    - Добавляет отсутствующие 'id' с нулевыми значениями повторов.\n",
    "    - Добавляет новый признак 'pre_since_opened_repeated_prop' в df_to_update,\n",
    "      нормируя сумму повторов на количество записей 'rn_max' для каждого 'id'.\n",
    "\n",
    "    Args:\n",
    "        df :  Исходный DataFrame с признаками  'pre_since_opened', 'id' и 'rn_max'.\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame :  Копия DataFrame с \n",
    "        добавленным признаком 'pre_since_opened_repeated_prop'.\n",
    "    \"\"\"\n",
    "    \n",
    "    print('FUNCTION from_pre_since_opened_create_pre_since_opened_sum_mean_repeated ')\n",
    "\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Считаем количество каждого значения 'pre_since_opened' для каждого 'id'\n",
    "    counts = df.groupby(['id', 'pre_since_opened']).size()\n",
    "    \n",
    "    \"\"\"\n",
    "    Оставляем только повторяющиеся значения (количество > 1), \n",
    "    вычитаем первое появление.\n",
    "    \"\"\"\n",
    "    repeated_pre_since_opened = counts[counts > 1] - 1\n",
    "\n",
    "    # Суммируем количество повторов по каждому 'id'\n",
    "    sum_repeated = repeated_pre_since_opened.groupby('id').sum()\n",
    "    \n",
    "    # Добавляем отсутствующие 'id' с нулевыми значениями повторов\n",
    "    all_sum_repeated = sum_repeated.reindex(df['id'].unique(), fill_value=0)\n",
    "    \n",
    "    # Переименовываем Series для дальнейшего слияния\n",
    "    all_sum_repeated = all_sum_repeated.rename('pre_since_opened_repeated_prop')\n",
    "\n",
    "    # Объединяем с исходным DataFrame по 'id'\n",
    "    df = df.merge(all_sum_repeated, on='id', how='left')\n",
    "\n",
    "    # Нормируем сумму повторов на количество записей 'rn_max' для каждого 'id'\n",
    "    df['pre_since_opened_repeated_prop'] = (\n",
    "        df['pre_since_opened_repeated_prop'] / df['rn_max']\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "def drop_columns_drop_duplicates_pipeline(\n",
    "    df: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Удаляет исходные и временные признаки из DataFrame,\n",
    "    а также удаляет дубликаты по столбцу 'id', оставляя только первую запись.\n",
    "    После удаления дубликатов столбец 'id' также удаляется.\n",
    "\n",
    "    Args:\n",
    "        df : Исходный DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame : Копия DataFrame без указанных столбцов и дубликатов по 'id'.\n",
    "    \"\"\"\n",
    "\n",
    "    print('FUNCTION drop_columns_drop_duplicates ')\n",
    "    # Список столбцов на удаление\n",
    "    columns = [\n",
    "        'rn',\n",
    "        'pre_since_opened',\n",
    "        'pre_since_confirmed',\n",
    "        'pre_pterm',\n",
    "        'pre_fterm',\n",
    "        'pre_till_pclose',\n",
    "        'pre_till_fclose',\n",
    "        'pre_loans_credit_limit',\n",
    "        'pre_loans_next_pay_summ',\n",
    "        'pre_loans_outstanding',\n",
    "        'pre_loans_max_overdue_sum',\n",
    "        'pre_loans_credit_cost_rate',\n",
    "        'pre_loans5',\n",
    "        'pre_loans530',\n",
    "        'is_zero_loans5',\n",
    "        'is_zero_loans530',\n",
    "        'pre_util',\n",
    "        'pre_over2limit',\n",
    "        'is_zero_over2limit',\n",
    "        'enc_paym_0',\n",
    "        'enc_paym_1',\n",
    "        'enc_paym_2',\n",
    "        'enc_paym_8',\n",
    "        'enc_paym_9',\n",
    "        'enc_paym_10',\n",
    "        'enc_paym_24',\n",
    "        'enc_loans_account_holder_type',\n",
    "        'enc_loans_credit_status',\n",
    "        'enc_loans_credit_type',\n",
    "        'enc_loans_account_cur',\n",
    "        'is_zero_loans3060',\n",
    "        'is_zero_loans6090',\n",
    "        'is_zero_loans90',\n",
    "        'enc_paym_3',\n",
    "        'enc_paym_4',\n",
    "        'enc_paym_5',\n",
    "        'enc_paym_6',\n",
    "        'enc_paym_7',\n",
    "        'enc_paym_11',\n",
    "        'enc_paym_12',\n",
    "        'enc_paym_13',\n",
    "        'enc_paym_14',\n",
    "        'enc_paym_15',\n",
    "        'enc_paym_16',\n",
    "        'enc_paym_17',\n",
    "        'enc_paym_18',\n",
    "        'enc_paym_19',\n",
    "        'enc_paym_20',\n",
    "        'enc_paym_21',\n",
    "        'enc_paym_22',\n",
    "        'enc_paym_23',\n",
    "        'enc_paym_avg_1_all',\n",
    "        'enc_paym_avg_2_all',\n",
    "        'enc_paym_avg_0_this_year',\n",
    "        'enc_paym_avg_1_this_year',\n",
    "        'enc_paym_avg_0_last_year',\n",
    "        'is_zero_loans3060_prop_1',\n",
    "        'is_zero_loans6090_prop_1',\n",
    "        'is_zero_loans90_prop_1'\n",
    "    ]\n",
    "    df = df.copy()\n",
    "    \n",
    "    df = df.drop(columns, axis=1)\n",
    "    \n",
    "    \"\"\"\n",
    "    Удаляем дубликаты по столбцу 'id', оставляя первую запись\n",
    "    и сбрасываем индекс.\n",
    "    \"\"\"\n",
    "    df = df.drop_duplicates(subset=['id'], keep='first').reset_index(drop=True)\n",
    "    \n",
    "    # Удаляем столбец 'id', так как он больше не нужен\n",
    "    df = df.drop('id', axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53162a0f-0907-4ca3-8ec2-55ade8b74584",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Из-за большого размера датасета вычисление медиан признаков занимает\n",
    "большой объём памяти, что приводит к падению ядра ноутбука. Поэтому применим\n",
    "кастомный imputer и будем расчитывать медианы на 10% процентах датасета.\n",
    "Оценки медианы будут приближены к реальным медианам, но не совпадать с ними.\n",
    "Такое решение это компромис между точностью и производительностью.\n",
    "Например для признака id погрешность между реальной медианой и оценочной \n",
    "составила около 0.05%.\n",
    "\"\"\"\n",
    "class SampleMedianImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, sample_frac=0.1):\n",
    "        # Доля выборки для вычисления медиан\n",
    "        self.sample_frac = sample_frac\n",
    "        # Атрибут для хранения медиан\n",
    "        self.medians_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Создаём подвыборку датасета, random_state для воспроизводимости\n",
    "        sample = X.sample(frac=self.sample_frac, random_state=0)\n",
    "        # Вычисляем и сохраняем медианы\n",
    "        self.medians_ = sample.median()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Заполняем пропуски медианами\n",
    "        return X.fillna(self.medians_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3417c72d-5480-4485-bf36-277a44b44208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаём SampleMedianImputer, долю выборки оставляем равной 0.1\n",
    "imputer = SampleMedianImputer(sample_frac=0.1)\n",
    "\n",
    "# Создаём паплайн препроцессинга\n",
    "preprocessing_pipe = Pipeline([\n",
    "    ('to_numeric', FunctionTransformer(convert_all_to_numeric)), \n",
    "    ('imputer', imputer),\n",
    "    ('to_int', FunctionTransformer(lambda df: df.astype(int), validate=False)),\n",
    "    ('drop_duplicates', FunctionTransformer(lambda df: df.drop_duplicates(), validate=False))\n",
    "])\n",
    "\n",
    "# Создаём основной пайплайн\n",
    "main_pipe = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            'preprocessing',\n",
    "            preprocessing_pipe\n",
    "        ),\n",
    "        (\n",
    "            'create_rn_max_feature',\n",
    "            FunctionTransformer(create_rn_max_feature_pipeline)\n",
    "        ),\n",
    "        (\n",
    "            'enc_paym_transcoding', \n",
    "            FunctionTransformer(enc_paym_transcoding_pipeline)\n",
    "        ),\n",
    "        (\n",
    "            'create_definite_value_proportion_features',\n",
    "            FunctionTransformer(create_definite_value_proportion_features_pipeline)\n",
    "        ),\n",
    "        (\n",
    "            'create_sum_prop_1_feature',\n",
    "            FunctionTransformer(from_is_zero_prop_1_create_sum_prop_1_feature_pipeline)\n",
    "        ),\n",
    "        (\n",
    "            'create_mean_value_frequency_feature',\n",
    "            FunctionTransformer(create_mean_value_frequency_feature_pipeline)\n",
    "        ),\n",
    "        (\n",
    "            'from_enc_paym_create_normalized_group_sum_features_then_diff_features',\n",
    "            FunctionTransformer(enc_paym_norm_group_sum_diff_pipeline)\n",
    "        ),\n",
    "        (\n",
    "            'from_pre_since_opened_create_pre_since_opened_sum_mean_repeated',\n",
    "            FunctionTransformer(pre_since_opened_sum_mean_repeated_pipeline)\n",
    "        ),\n",
    "        (\n",
    "            'drop_temporary_and_source_columns_drop_duplicates',\n",
    "            FunctionTransformer(drop_columns_drop_duplicates_pipeline)\n",
    "        ),\n",
    "\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8659654c-4732-4087-a77b-42e9d384c3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FUNCTION create_rn_max_feature \n",
      "FUNCTION enc_paym_transcoding \n",
      "FUNCTION create_definite_value_proportion_features \n",
      "Исходный признак enc_loans_account_holder_type\n",
      "Новые фичи\n",
      "enc_loans_account_holder_type_prop_4\n",
      "Исходный признак pre_pterm\n",
      "Новые фичи\n",
      "pre_pterm_prop_6\n",
      "pre_pterm_prop_3\n",
      "Исходный признак is_zero_loans530\n",
      "Новые фичи\n",
      "is_zero_loans530_prop_1\n",
      "Исходный признак enc_paym_0\n",
      "Новые фичи\n",
      "enc_paym_0_prop_1\n",
      "Исходный признак pre_loans_credit_cost_rate\n",
      "Новые фичи\n",
      "pre_loans_credit_cost_rate_prop_6\n",
      "pre_loans_credit_cost_rate_prop_11\n",
      "pre_loans_credit_cost_rate_prop_4\n",
      "Исходный признак pre_loans_next_pay_summ\n",
      "Новые фичи\n",
      "pre_loans_next_pay_summ_prop_5\n",
      "pre_loans_next_pay_summ_prop_0\n",
      "Исходный признак is_zero_over2limit\n",
      "Новые фичи\n",
      "is_zero_over2limit_prop_1\n",
      "Исходный признак pre_loans_outstanding\n",
      "Новые фичи\n",
      "pre_loans_outstanding_prop_1\n",
      "pre_loans_outstanding_prop_5\n",
      "Исходный признак pre_util\n",
      "Новые фичи\n",
      "pre_util_prop_3\n",
      "pre_util_prop_6\n",
      "Исходный признак pre_till_pclose\n",
      "Новые фичи\n",
      "pre_till_pclose_prop_10\n",
      "pre_till_pclose_prop_7\n",
      "Исходный признак is_zero_loans5\n",
      "Новые фичи\n",
      "is_zero_loans5_prop_1\n",
      "Исходный признак pre_since_confirmed\n",
      "Новые фичи\n",
      "pre_since_confirmed_prop_4\n",
      "pre_since_confirmed_prop_7\n",
      "Исходный признак pre_loans_credit_limit\n",
      "Новые фичи\n",
      "pre_loans_credit_limit_prop_2\n",
      "pre_loans_credit_limit_prop_15\n",
      "pre_loans_credit_limit_prop_18\n",
      "Исходный признак pre_over2limit\n",
      "Новые фичи\n",
      "pre_over2limit_prop_17\n",
      "Исходный признак pre_till_fclose\n",
      "Новые фичи\n",
      "pre_till_fclose_prop_4\n",
      "pre_till_fclose_prop_3\n",
      "pre_till_fclose_prop_1\n",
      "Исходный признак enc_loans_credit_status\n",
      "Новые фичи\n",
      "enc_loans_credit_status_prop_5\n",
      "Исходный признак pre_since_opened\n",
      "Новые фичи\n",
      "pre_since_opened_prop_12\n",
      "pre_since_opened_prop_8\n",
      "pre_since_opened_prop_19\n",
      "Исходный признак enc_paym_24\n",
      "Новые фичи\n",
      "enc_paym_24_prop_1\n",
      "Исходный признак pre_loans_max_overdue_sum\n",
      "Новые фичи\n",
      "pre_loans_max_overdue_sum_prop_1\n",
      "Исходный признак enc_loans_credit_type\n",
      "Новые фичи\n",
      "enc_loans_credit_type_prop_0\n",
      "enc_loans_credit_type_prop_2\n",
      "Исходный признак pre_fterm\n",
      "Новые фичи\n",
      "pre_fterm_prop_7\n",
      "pre_fterm_prop_3\n",
      "Исходный признак is_zero_loans3060\n",
      "Новые фичи\n",
      "is_zero_loans3060_prop_1\n",
      "Исходный признак is_zero_loans6090\n",
      "Новые фичи\n",
      "is_zero_loans6090_prop_1\n",
      "Исходный признак is_zero_loans90\n",
      "Новые фичи\n",
      "is_zero_loans90_prop_1\n",
      "FUNCTION of_is_zero_prop_1_create_sum_prop_1_feature \n",
      "FUNCTION create_mean_value_frequency_feature \n",
      "new_column pre_util_mean_freq\n",
      "new_column pre_loans_credit_limit_mean_freq\n",
      "new_column pre_since_opened_mean_freq\n",
      "new_column pre_loans_credit_cost_rate_mean_freq\n",
      "new_column enc_loans_credit_type_mean_freq\n",
      "new_column pre_loans_next_pay_summ_mean_freq\n",
      "new_column pre_since_confirmed_mean_freq\n",
      "new_column pre_pterm_mean_freq\n",
      "new_column enc_paym_0_mean_freq\n",
      "new_column enc_loans_account_holder_type_mean_freq\n",
      "new_column pre_loans530_mean_freq\n",
      "new_column enc_paym_8_mean_freq\n",
      "new_column pre_loans5_mean_freq\n",
      "new_column enc_paym_10_mean_freq\n",
      "new_column enc_loans_account_cur_mean_freq\n",
      "new_column enc_paym_9_mean_freq\n",
      "FUNCTION enc_paym_norm_group_sum_diff_pipeline \n",
      "new_column enc_paym_avg_1_all\n",
      "new_column enc_paym_avg_2_all\n"
     ]
    }
   ],
   "source": [
    "# Обучим пайплайн\n",
    "X_train_full = main_pipe.fit_transform(X_train_full)\n",
    "X_train_full"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10464883-970a-4a9e-a3b3-4c72b0a863db",
   "metadata": {},
   "source": [
    "git commit -m \"Change SimpleImputer to custom imputer\" -m \"-Add BaseEstimator, TransformerMixin imports\n",
    "-Create SampleMedianImputer\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
